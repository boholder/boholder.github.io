<!DOCTYPE html>
<html lang="en-US">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.111.3"><meta name="theme-color" content="#fff" />
    <meta name="color-scheme" content="light dark">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>Train a LoRA model for an anime character | BoHolder&#39;s site: crafts, blogs etc</title>

    <link rel="stylesheet" href="/en-us/css/meme.min.8463164c1364c4b1e727693b9d728e76cea236cbeb4d1e0a285a5b86ad8a5193.css"/>

    
    
        <script src="/en-us/js/meme.min.88dcafd9937dec27b87f333937e9be42cdffb410dd0317d810f064321461f0ad.js"></script>

    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" /></noscript>

    <meta name="author" content="BoHolder" /><meta name="description" content="From data collecting to training" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="BoHolder&#39;s site: crafts, blogs etc" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="BoHolder&#39;s site: crafts, blogs etc" />
    <meta name="msapplication-starturl" content="../../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
    <link rel="canonical" href="https://boholder.github.io/en-us/blogs/train-a-lora-model-for-an-anime-character/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "datePublished": "2023-03-14T00:00:00+00:00",
        "dateModified": "2023-03-25T14:23:00+08:00",
        "url": "https://boholder.github.io/en-us/blogs/train-a-lora-model-for-an-anime-character/",
        "headline": "Train a LoRA model for an anime character",
        "description": "From data collecting to training",
        "inLanguage" : "en-US",
        "articleSection": "blogs",
        "wordCount":  2610 ,
        "image": ["https://boholder.github.io/img/postimg/maho_by_ai.png"],
        "author": {
            "@type": "Person",
            "description": "Slow is smooth, smooth is fast.",
            "email": "bottleholder@anche.no",
            "image": "https://boholder.github.io/icons/apple-touch-icon.png",
            "url": "https://boholder.github.io/",
            "name": "BoHolder"
        },
        "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)",
        "publisher": {
            "@type": "Organization",
            "name": "BoHolder's site: crafts, blogs etc",
            "logo": {
                "@type": "ImageObject",
                "url": "https://boholder.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://boholder.github.io/en-us/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://boholder.github.io/en-us/"
        }
    }
</script>

    

<meta name="twitter:card" content="summary_large_image" />



    



<meta property="og:title" content="Train a LoRA model for an anime character" />
<meta property="og:description" content="From data collecting to training" />
<meta property="og:url" content="https://boholder.github.io/en-us/blogs/train-a-lora-model-for-an-anime-character/" />
<meta property="og:site_name" content="BoHolder&#39;s site: crafts, blogs etc" />
<meta property="og:locale" content="en-us" />
            <meta property="og:locale:alternate" content="zh-cn" />
        <meta property="og:image" content="https://boholder.github.io/img/postimg/maho_by_ai.png" />
<meta property="og:type" content="article" />
    <meta property="article:published_time" content="2023-03-14T00:00:00&#43;00:00" />
    <meta property="article:modified_time" content="2023-03-25T14:23:00&#43;08:00" />
    
    <meta property="article:section" content="blogs" />



    
    

    
</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            
    <div class="site-brand">
        
            <a href="/en-us/" class="brand">BoHolder&#39;s site: crafts, blogs etc</a>
        
    </div>

        
    </header>




            
                
                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item"><a href="/en-us/blogs/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon blogs"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg><span class="menu-item-name">Blogs</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/en-us/categories/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon th"><path d="M149.333 56v80c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V56c0-13.255 10.745-24 24-24h101.333c13.255 0 24 10.745 24 24zm181.334 240v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm32-240v80c0 13.255 10.745 24 24 24H488c13.255 0 24-10.745 24-24V56c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24zm-32 80V56c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm-205.334 56H24c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24zM0 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H24c-13.255 0-24 10.745-24 24zm386.667-56H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zm0 160H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zM181.333 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24z"/></svg><span class="menu-item-name">Categories</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/en-us/about/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 192 512" class="icon info"><path d="M20 424.229h20V279.771H20c-11.046 0-20-8.954-20-20V212c0-11.046 8.954-20 20-20h112c11.046 0 20 8.954 20 20v212.229h20c11.046 0 20 8.954 20 20V492c0 11.046-8.954 20-20 20H20c-11.046 0-20-8.954-20-20v-47.771c0-11.046 8.954-20 20-20zM96 0C56.235 0 24 32.235 24 72s32.235 72 72 72 72-32.235 72-72S135.764 0 96 0z"/></svg><span class="menu-item-name">About</span></a>
                </li>
            
        
    </ul>
</nav>

                
                <div id="lang-switcher">
            <span>English</span>
            
                <ul id="langs">
                    
                        <li><a rel="alternate" href="/blogs/train-a-lora-model-for-an-anime-character/" hreflang="zh-cn" lang="zh-cn">中文</a></li>
                    
                </ul>
            
        </div>
                <a id="theme-switcher" href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5l48.8-97.5a18 18 0 0128 0l48.8 97.5 103.4 -34.5a18 18 0 0119.8 19.8l-34.5 103.4l97.5 48.8a18 18 0 010 28l-97.5 48.8 34.5 103.4a18 18 0 01-19.8 19.8l-103.4-34.5-48.8 97.5a18 18 0 01-28 0l-48.8-97.5l-103.4 34.5a18 18 0 01-19.8-19.8l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8-34.5-103.4a18 18 0 0119.8-19.8zM256 128a128 128 0 10.01 0M256 160a96 96 0 10.01 0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412a256 256 0 10154-407a11.5 11.5 0 00-5 20a201.5 201.5 0 01-134 374a11.5 11.5 0 00-15 13"/></svg></a>
            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-align="justify" data-type="blogs" data-toc-num="true">

            <h1 class="post-title p-name">Train a LoRA model for an anime character</h1>

            

            
                <div class="post-description p-summary">From data collecting to training</div>
                
            

            
                

<div class="post-meta">
    
        
        <time datetime="2023-03-14T00:00:00&#43;00:00" class="post-meta-item published dt-published"><svg class="icon post-meta-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 44 40"><path d="M40 8H8c-2.21 0-3.98 1.79-3.98 4L4 36c0 2.21 1.79 4 4 4h32c2.21 0 4-1.79 4-4V12c0-2.21-1.79-4-4-4zM17 30h-2.4l-5.1-7v7H7V18h2.5l5 7v-7H17v12zm10-9.49h-5v2.24h5v2.51h-5v2.23h5V30h-8V18h8v2.51zM41 28c0 1.1-.9 2-2 2h-8c-1.1 0-2-.9-2-2V18h2.5v9.01h2.25v-7.02h2.5v7.02h2.25V18H41v10z"></path></svg>&nbsp;2023.3.14</time>
    
    
        
        <time datetime="2023-03-25T14:23:00&#43;08:00" class="post-meta-item modified dt-updated"><svg class="icon post-meta-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 45 45"><path d="M44.4 36.9L26.1 18.6c1.8-4.6.8-10.1-2.9-13.8-4-4-10-4.8-14.8-2.5l8.7 8.7-6.1 6.1-8.7-8.7C0 13.2.8 19.2 4.8 23.2c3.7 3.7 9.2 4.7 13.8 2.9l18.3 18.3c.8.8 2.1.8 2.8 0l4.7-4.7c.8-.7.8-2 0-2.8z"></path></svg>&nbsp;2023.3.25</time>
    
    
    
        
        
        
            
                <span class="post-meta-item category"><svg class="icon post-meta-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 36 36"><path d="M16 0H4C1.79 0 .02 1.79.02 4L0 28c0 2.21 1.79 4 4 4h32c2.21 0 4-1.79 4-4V8c0-2.21-1.79-4-4-4H20l-4-4z"></path></svg>&nbsp;<a href="/en-us/categories/fun/" class="category-link p-category">fun</a></span>
            
        
    
    
        
        <span class="post-meta-item wordcount"><svg class="icon post-meta-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 36 36"><path d="M0 28.69v7.5h7.5l22.13-22.13-7.5-7.5L0 28.69zM35.41 8.28c.78-.78.78-2.05 0-2.83L30.74.78c-.78-.78-2.05-.78-2.83 0l-3.66 3.66 7.5 7.5 3.66-3.66z"></path></svg>&nbsp;2610</span>
    
    
        
        <span class="post-meta-item reading-time"><svg class="icon post-meta-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><path d="M19.99 0C8.94 0 0 8.95 0 20s8.94 20 19.99 20C31.04 40 40 31.05 40 20S31.04 0 19.99 0zM20 36c-8.84 0-16-7.16-16-16S11.16 4 20 4s16 7.16 16 16-7.16 16-16 16z"/><path d="M21 10h-3v12l10.49 6.3L30 25.84l-9-5.34z"></path></svg>&nbsp;13&nbsp;mins</span>
    
    
    
</div>

            

            <nav class="contents">
  <h2 id="contents" class="contents-title">Contents</h2><ol class="toc">
    <li><a id="contents:some-pre-knowledge-for-better-understanding" href="#some-pre-knowledge-for-better-understanding">Some pre-knowledge for better understanding</a></li>
    <li><a id="contents:prepare-the-dataset" href="#prepare-the-dataset">Prepare the dataset</a>
      <ol>
        <li><a id="contents:collecting-pictures" href="#collecting-pictures">Collecting pictures</a></li>
        <li><a id="contents:remove-pictures-background" href="#remove-pictures-background">Remove pictures background</a></li>
        <li><a id="contents:anti-pattern-enlarge-the-image-size-to-square" href="#anti-pattern-enlarge-the-image-size-to-square">Anti-pattern: Enlarge the image size to square</a></li>
        <li><a id="contents:crop-the-face-out" href="#crop-the-face-out">Crop the face out</a></li>
        <li><a id="contents:resize-and-caption" href="#resize-and-caption">Resize and Caption</a></li>
        <li><a id="contents:check-and-modify-the-captions" href="#check-and-modify-the-captions">Check and modify the captions</a>
          <ol>
            <li><a id="contents:for-training" href="#for-training">For training</a></li>
            <li><a id="contents:for-usage" href="#for-usage">For usage</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a id="contents:training" href="#training">Training</a></li>
    <li><a id="contents:conclusion" href="#conclusion">Conclusion</a></li>
  </ol>
</nav><div class="post-body e-content">
                <p><em>This article was completed with the help of <a href="https://www.DeepL.com/Translator" target="_blank" rel="noopener">www.DeepL.com/Translator</a>.</em></p>
<p>I saw the <a href="https://civitai.com/models/16104/holo-spice-and-wolf" target="_blank" rel="noopener">LoRA model</a> of Hero from &ldquo;Wolf and Spice&rdquo; posted by <a href="https://twitter.com/fuyufjh" target="_blank" rel="noopener">Eric Fu</a> earlier. My heart ignited an impulse, who does not want to refine a model for waifu? So&hellip; I tried to make it done. The process was recorded for your reference.</p>
<p>To complete this thing requires you have:</p>
<ul>
<li>
<p>A certain understanding of Python coding and Python environment building, ML projects or old projects inevitably have a bit of dependency conflicts. In order to avoid dependency confusion, I have prepared a separate venv for each tool.</p>
</li>
<li>
<p>If your computer&rsquo;s GPU is Nvidia supported CUDA GPU, remember to install pytorch-cuda before installing dependencies for each project, this package uses GPU while pytorch will drive your CPU crying. Installation reference <a href="https://github.com/openai/whisper/discussions/47" target="_blank" rel="noopener">this discussion</a>.</p>
</li>
<li>
<p>A nice computer, even if your hardware is not enough for the final fine-tuning, and you have to run it on the cloud with <a href="https://colab.research.google.com/" target="_blank" rel="noopener">Colab</a> or something similar, it&rsquo;s more convenient if you can run the dataset preprocessing locally.</p>
<ul>
<li>BTW I ended up refining the model locally, with pytorch-cuda installed, the torch still choose to run with the CPU, not sure what is going on but there are benefits: slow but no need to worry about memory overflow.</li>
<li>For your Reference: I got an AMD 3700X CPU with 3600 Mhz, the fine-tuning takes about 40% load of it. I&rsquo;ve forgotten the memory consumption but it definitely less than 10GB.</li>
</ul>
</li>
</ul>
<p>Tools/projects used:</p>
<ul>
<li>The famous <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noopener">stable-diffusion-webui</a> for resizing and auto-tagging the dataset.</li>
<li>Model：<a href="https://github.com/SkyTNT/anime-segmentation" target="_blank" rel="noopener">anime-segmentation - SkyTNT</a> for removing background of character.</li>
<li>Model：<a href="https://github.com/qhgz2013/anime-face-detector" target="_blank" rel="noopener">anime-face-detector - qhgz2013</a> for face recognizing and cropping to create some face-focused training picture.</li>
<li>Use the <a href="https://github.com/Akegarasu/lora-scripts" target="_blank" rel="noopener">lora-scripts - Akegarasu</a>to control the <a href="https://github.com/kohya-ss/sd-scripts" target="_blank" rel="noopener">sd-scripts - kohya-ss</a>to train the <a href="https://github.com/KohakuBlueleaf/LyCORIS" target="_blank" rel="noopener">LyCORIS (LoCon-improvement?)</a> Model (the author of the model is a really fun).</li>
</ul>
<p>Also note that models have their own specialties, the tools I use in this process are for anime characters, if you refine real people (for example, <a href="https://huggingface.co/swl-models/chilloutmix" target="_blank" rel="noopener">Chilloutmax</a> as the base model), you should find other tools for the same purpose. If we want to train an embedding (textual inversion) instead of a LoRA model, the form of the required dataset will be different from this article.</p>
<h2 id="some-pre-knowledge-for-better-understanding"><a href="#some-pre-knowledge-for-better-understanding" class="anchor-link">#</a><a href="#contents:some-pre-knowledge-for-better-understanding" class="headings">Some pre-knowledge for better understanding</a></h2>
<ul>
<li>
<p>Stable Diffusion is a text-img model that learns how to denoise noisy images in reverse by observing the gradual addition of noise to the images.</p>
<ul>
<li>article: <a href="https://www.paepper.com/blog/posts/how-and-why-stable-diffusion-works-for-text-to-image-generation/" target="_blank" rel="noopener">How and why stable diffusion works for text to image generation</a></li>
<li>video: <a href="https://www.youtube.com/watch?v=-lz30by8-sU" target="_blank" rel="noopener">Stable Diffusion in Code (AI Image Generation) - Computerphile</a></li>
</ul>
</li>
<li>
<p>LoRA is a model fine-tune technique.</p>
<ul>
<li>article: <a href="https://huggingface.co/blog/lora" target="_blank" rel="noopener">Hugging face: Using LoRA for Efficient Stable Diffusion Fine-Tuning</a></li>
<li>Comparison of several fine-tuning techniques: video: <a href="https://www.youtube.com/watch?v=dVjMiJsuR5o" target="_blank" rel="noopener">LoRA vs Dreambooth vs Textual Inversion vs Hypernetworks</a>, don&rsquo;t want to see the video? here is <a href="https://www.reddit.com/r/StableDiffusion/comments/10cgxrx/wellresearched_comparison_of_training_techniques/" target="_blank" rel="noopener">a visual comparison chart</a></li>
</ul>
</li>
<li>
<p><a href="https://github.com/kohya-ss/sd-scripts/blob/main/train_README-ja.md#%E5%AD%A6%E7%BF%92%E3%81%A7%E4%BD%BF%E3%82%8F%E3%82%8C%E3%82%8B%E7%94%A8%E8%AA%9E%E3%81%AE%E3%81%94%E3%81%8F%E7%B0%A1%E5%8D%98%E3%81%AA%E8%A7%A3%E8%AA%AC" target="_blank" rel="noopener">学習で使われる用語のごく簡単な解説</a></p>
</li>
</ul>
<p>In addition to these &ldquo;basic knowledge&rdquo;, when you see terms that you do not understand, just search for concepts for about five minutes, and these terms will gradually become a system of knowledge. For now most of the parameters for controlling AI-generation are jargon, so in order to figure out what values are appropriate to set, we have to learn their meanings and operate on the basis of understanding. I found that <code>&quot;&lt;noun you don't know&gt;&quot; [ML|stable diffusion|fine-tune|lora]</code> keyword search works well (only search nouns are easily confused by the more general meaning of the search results, adding a domain word is much better).</p>
<h2 id="prepare-the-dataset"><a href="#prepare-the-dataset" class="anchor-link">#</a><a href="#contents:prepare-the-dataset" class="headings">Prepare the dataset</a></h2>
<h3 id="collecting-pictures"><a href="#collecting-pictures" class="anchor-link">#</a><a href="#contents:collecting-pictures" class="headings">Collecting pictures</a></h3>
<p>First I found the article <a href="https://bennycheung.github.io/stable-diffusion-training-for-embeddings" target="_blank" rel="noopener">Stable Diffusion Training for Personal Embedding</a>, which mentioned <a href="https://www.youtube.com/watch?v=P1dfwViVOIU" target="_blank" rel="noopener">this video</a> about training a personal portrait model requires:</p>
<ul>
<li>three different angled full body pictures</li>
<li>five different angled half body pictures</li>
<li>and twelve different angled face pictures</li>
</ul>
<p>Preparing this dataset is as simple as finding a white wall and taking selfies, since the model is yourself anyway. Virtual characters can&rsquo;t pose for me&hellip;&hellip; Where to find the pictures? This depends on the character, whether the carrier work has multiple multimedia derivatives, and whether the character is popular&hellip;&hellip; all have an impact. For example, Eric Fu&rsquo;s Hero used illustrations from light novels and comics.</p>
<p>The character I want to train is a less popular character, and there are not many &ldquo;official pictures&rdquo; that I can cut. I learned from Eric Fu that &ldquo;the setting needs to be close, and the drawing style can be varied&rdquo;, so&hellip;&hellip; I opened the collection directory of fan art about her, plus the &ldquo;official pictures&rdquo; finally I picked about a hundred pictures (what, don&rsquo;t have a collection directory for your waifu? Your love is not enough hahaha). From the results, it still works well to use images with different drawing styles for the dataset.</p>
<p><strong>Although I think it&rsquo;s not a big deal to play with yourself, but using fan arts to train AI is a cardinal sin in some people&rsquo;s view. To be clear, if you train with the fan arts, do not make a statement, do not publish the model, I used it only because I have no choice.</strong></p>
<p>I do not know if it is appropriate, my personal criteria for selecting pictures is &ldquo;obvious features&rdquo;, fine-tuning is letting the model learn your target&rsquo;s features after all. For example:</p>
<ul>
<li>✔️ character dresses &ldquo;same with the official settings&rdquo; (most of the pictures are qualified).</li>
<li>✔️ character dresses non-official-setting, but hair, facial details, etc. &ldquo;very similar to the official&rdquo;.</li>
<li>✔️ multiplayer pictures, if you don&rsquo;t want to waste them, cut your character out.</li>
<li>❌ the environment (light and shadow) causes a big impact on the character&rsquo;s color.</li>
<li>❌ The artist&rsquo;s drawing style is unique, or too good to be used.</li>
<li>❌ Non-color images.</li>
</ul>
<h3 id="remove-pictures-background"><a href="#remove-pictures-background" class="anchor-link">#</a><a href="#contents:remove-pictures-background" class="headings">Remove pictures background</a></h3>
<p>First I observed the portraits of the two who training for personal portrait Embedding, and it seems that the white background was purposely selected for the selfies. Then I watched the video <a href="https://www.youtube.com/watch?v=WO2X3oZEJOA" target="_blank" rel="noopener">Glitch Tokens - Computerphile</a>, which mentions that researchers found that the feature learned by a certain model for dumbbell text was an upper arm with muscles. These made me speculate that <strong>image backgrounds are a noise to character feature learning</strong>, so I
decided to remove the backgrounds. (Lately I learned that we can use <a href="https://github.com/rinongal/textual_inversion/issues/131" target="_blank" rel="noopener">caption</a> to subtract certain features from the image so that the model doesn&rsquo;t learn them, but I don&rsquo;t think that work as well as just removing the background.)</p>
<p>It&rsquo;s hard to find an automatic background removal tool with ordinary internet users of the keyword <code>photo remove background</code>, the vast number of sites want to earn your money. I found a Photoshop script, but the effect is not satisfactory. A friend recommended this project <a href="https://github.com/SkyTNT/anime-segmentation" target="_blank" rel="noopener">anime-segmentation - SkyTNT</a>, which led me to <a href="https://www.tensorflow.org/tutorials/images/segmentation" target="_blank" rel="noopener">image segmentation</a> as an ML field, and the results of this project given by my friend have been very good. Thank you, my friend!</p>
<p>Built it successfully with Python 3.9, be aware that it only seems to support png and jpg format input. This converted our original image to a transparent background image of just the people in png format. Some of the output may not have been removed cleanly, and I didn&rsquo;t bother to remove it manually again myself.</p>
<h3 id="anti-pattern-enlarge-the-image-size-to-square"><a href="#anti-pattern-enlarge-the-image-size-to-square" class="anchor-link">#</a><a href="#contents:anti-pattern-enlarge-the-image-size-to-square" class="headings">Anti-pattern: Enlarge the image size to square</a></h3>
<p>First I experimented the resize operation and found that this would truncate the corner parts of the picture. But I didn&rsquo;t want to lose any features (especially the rare official-setting shoes), so I found a <a href="https://community.adobe.com/t5/photoshop-ecosystem-discussions/canvas-resize-to-square-for-a-large-number-of-images-using-script/td-p/6148635" target="_blank" rel="noopener">Photoshop script</a> on the internet, and expanded all the images into squares according to the larger one in length and width, so that resizing result would remain the whole image. This step proves to be <strong>useless</strong> and makes the features smaller and fuzzier, which is not conducive to training.</p>
<h3 id="crop-the-face-out"><a href="#crop-the-face-out" class="anchor-link">#</a><a href="#contents:crop-the-face-out" class="headings">Crop the face out</a></h3>
<p>According to the portrait dataset, the face (up to the shoulder) picture accounts for quite a bit. But the fan artists will certainly not just draw an ID photo, it seems that we need the face pictures out from the fan arts (transparent background). After having tasted the sweetness of dealing things in ML, I directly searched <code>anime face detect ML</code>, and Github <a href="https://github.com/search?q=anime+face+detect" target="_blank" rel="noopener">gives out some projects</a>.</p>
<p>Picking a project, especially a Python ML project (to write some code based on the project to crop out the image and save it), I care more about the development time than the star count, afraid of the age of the project dependencies may have problems. <a href="https://github.com/hysts/anime-face-detector" target="_blank" rel="noopener">anime-face-detector - hysts</a>, it&rsquo;s the newest one, developed two years ago, but it took me a long time to solve the versioning problem about <code>mmcv</code> dependencies, finally I failed to run it up. Moved to the second newest (developed three years ago) project <a href="https://github.com/qhgz2013/anime-face-detector" target="_blank" rel="noopener">anime-face-detector - qhgz2013</a>.</p>
<p>Wow, it works on Python 3.9, so I forked the project and <a href="https://github.com/boholder/anime-face-detector/commit/afc906d49fe03aeaea457ae4d0f0523d09b62a57" target="_blank" rel="noopener">changed a couple of lines</a> to make cropped out picture larger than the facial recognition results, including the hair and shoulders and animal ears (animal ears up the long). Thanks qhgz2013 has developed the crop function, save me the trouble. By this way the face is cut out from each picture and the training set size is doubled.</p>
<h3 id="resize-and-caption"><a href="#resize-and-caption" class="anchor-link">#</a><a href="#contents:resize-and-caption" class="headings">Resize and Caption</a></h3>
<p>Now that the dataset is prepared, open the web-ui -&gt; Train -&gt; Preprocess images, and follow the steps in <a href="https://dfldata.xyz/forum.php?mod=viewthread&amp;tid=12796" target="_blank" rel="noopener">this tutorial</a> to:</p>
<ul>
<li>
<p>Resize the images to 512*512 (although LoRA is supporting non-512 size, I didn&rsquo;t quite understand LoRA&rsquo;s requirements for training set images, so for insurance just follow the classic 512 (the input image size for stable diffusion v1.* version)).</p>
</li>
<li>
<p>Check the <code>Auto focal point crop</code></p>
</li>
<li>
<p>Check the <code>Use Deepbooru for caption</code> to use <a href="https://github.com/KichangKim/DeepDanbooru" target="_blank" rel="noopener">DeepDanbooru - KichangKim</a> - the image tag extraction model trained with anime characters. It automatically labels the training set, the set of these is called a caption, and as mentioned above, the purpose is to keep the model from learning (=reinforcing the weight of these tags) these annotated features and focus on learning the &ldquo;new&rdquo; (character) features.</p>
</li>
</ul>
<p>Now we finally have the training set ready, a bunch of 512*512 images with corresponding caption files. According to the results, the images generated by face-detect-model and web-ui both replace the transparent background with a pure black background, and the training set with black background does affect the generation of the resulting model (intend to generate pure black background). But if we give the background prompt, the generation backs to normal, so no need to worry about it.</p>
<h3 id="check-and-modify-the-captions"><a href="#check-and-modify-the-captions" class="anchor-link">#</a><a href="#contents:check-and-modify-the-captions" class="headings">Check and modify the captions</a></h3>
<p>FYI: <a href="https://github.com/rinongal/textual_inversion/issues/131" target="_blank" rel="noopener">Do image captions have any effect during training?</a></p>
<p>After further search and experimentation, I found caption to be of great use for training and using the model. I <a href="https://github.com/boholder/tools_for_training_sd_model" target="_blank" rel="noopener">wrote a few simple scripts</a> to do the related automation work. The character I trained is <a href="https://gamewith.jp/pricone-re/article/show/92907" target="_blank" rel="noopener">姫宮真歩 Himemiya Maho</a> from the mobile game &ldquo;Princess Connect! Re: Dive&rdquo;, using her as an example.</p>
<p><img src="/img/postimg/maho_by_ai.png" alt="an anime girl picture generated by AI"></p>
<div class="highlight"><div class="chroma">
<div class="table-container"><table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl"># The official image in the linked website is very similar to the image in the AI-generated diagram, I&#39;m pretty happy about this good result.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">(masterpiece:1.331), best_quality,
</span></span><span class="line"><span class="cl">(1girl:1.21),
</span></span><span class="line"><span class="cl">fox_ear, fox_tail,
</span></span><span class="line"><span class="cl">(brown|gold) hair, long_hair, hair_tubes, blunt_bangs,
</span></span><span class="line"><span class="cl">green_eyes, narrowed_eyes,
</span></span><span class="line"><span class="cl">medium_breasts,
</span></span><span class="line"><span class="cl">smile,
</span></span><span class="line"><span class="cl">print_kimono, frills, maid_headdress, hair_flower,
</span></span><span class="line"><span class="cl">sit,
</span></span><span class="line"><span class="cl">nature,
</span></span><span class="line"><span class="cl">looking_at_viewer,
</span></span><span class="line"><span class="cl">&lt;lora:himemiyamaho:0.7&gt;
</span></span></code></pre></td></tr></table></div>
</div>
</div><h4 id="for-training"><a href="#for-training" class="anchor-link">#</a><a href="#contents:for-training" class="headings">For training</a></h4>
<p><strong>By removing the character feature tags from the caption, the model can be reinforced to learn those features.</strong> Those features should be the ones that the character has in <strong>all</strong> scenes you want to generate in the future (e.g., Shinpachi Shimura&rsquo;s glasses). For example, costumes should not be removed because character costumes change from scene to scene. For Maho, I removed the following tags:</p>
<ul>
<li><code>blunt_bangs</code></li>
<li><code>eyebrows_visible_through_hair</code>: the so-called wrong-layer eyebrows, not obvious on Maho, other characters such as <a href="https://disp.cc/b/ACG/ftGJ" target="_blank" rel="noopener">Inuyama Aoi</a> is more obvious.</li>
<li><code>low_twintails</code></li>
<li><code>*_breasts</code>: Why remove the breasts tag? The initial version of the model trained without removing these tags generates 100% huge breasts without specifying the size of the breasts. Therefore, the tags is deleted to let the model learn the real size of Maho.</li>
</ul>
<p>I didn&rsquo;t remove very basic features such as <code>fox_ear, fox_tail, (brown|gold) hair, long_hair, green_eyes,</code> which, judging by my current knowledge base, are fine to be removed.</p>
<p><strong>If you have features you want to avoid the model to learn, add the corresponding tag in the caption of each image</strong>. For Maho, I deleted <code>bare_shoulders</code>. Why? I used two sets of clothes for training, the usual clothes (kimono as shown in the picture) and the swimsuit. In the initial version of the training model, I observed that the vast majority of the usual clothes generated pictures, are like the flower leader to show the shoulders, not in line with the impression of the role of Maho. At first, I thought the model had mixed it up with the swimsuit (with bare shoulders), but later I checked the training set and found that many of the usual clothes pictures I used were also strapless. I can not collect other enough qualified pictures for training, so how to solve this problem? Finally, I decide to use the caption to control the model learning, and experiment result shows that it worked well, and finally Maho would no longer showed her shoulders.</p>
<h4 id="for-usage"><a href="#for-usage" class="anchor-link">#</a><a href="#contents:for-usage" class="headings">For usage</a></h4>
<p>The caption helps the model understand the images in the training set as it learns. It is also very eye-opening for me, who has a poor English vocabulary, to learn a lot of new words that will help me communicate more accurately with the model.</p>
<p><strong>prompt for training pictures</strong> = <strong>caption</strong> (what the model doesn&rsquo;t learn) + <strong>what the model learns</strong></p>
<p>When it comes time to use it we can reverse this formula: tags in caption + model = what looks like the result of the training images! If you want to generate results similar to a certain image in the training set, look at its caption.</p>
<h2 id="training"><a href="#training" class="anchor-link">#</a><a href="#contents:training" class="headings">Training</a></h2>
<p>First of all, my fine-tune technology is to follow Eric Fu selected LoRA, because I heard that LoRA technology is not only fresher in the timeline, and seems to have low memory requirements. Then LoRA also divided into several algorithms, I chose the <a href="https://github.com/KohakuBlueleaf/LyCORIS" target="_blank" rel="noopener">LyCORIS</a> model as same as Eric Fu, take the road that others have verified, while I do not have enough ML knowledge to do the selection myself. The <code>LyCORIS</code>&rsquo;s README and The README of <a href="https://github.com/Akegarasu/lora-scripts" target="_blank" rel="noopener">lora-scripts - Akegarasu</a> project found in <a href="https://dfldata.xyz/forum.php?mod=viewthread&amp;tid=12796" target="_blank" rel="noopener">a tutorial in forum</a> both point to this project: <a href="https://github.com/kohya-ss/sd-scripts" target="_blank" rel="noopener">sd-scripts - kohya-ss</a>. The relationship between the three of them is:</p>
<ul>
<li><code>sd-scripts</code> integrates fine-tune scripts that are easy to use directly and can perform training with all four fine-tune methods.</li>
<li><code>lora-scripts</code> further simplifies the use of <code>sd-scripts</code> for LoRA training, and each parameter has a recommended value, which is a big help.</li>
<li>Run <code>sd-scripts</code> with parameters specifying <code>LyCORIS</code> as the training algorithm.</li>
</ul>
<p>With Python 3.10, first install <code>lora-scripts</code> (which will also install the <code>sd-scripts</code> dependency), then install <code>LyCORIS</code> library, which the venv is shared by all three projects. The base model I used is [anything-v4.5](<a href="https://huggingface.co/" target="_blank" rel="noopener">https://huggingface.co/</a> andite/anything-v4.0). I <a href="https://github.com/boholder/lora-scripts/commit/0780625ad726a0a03f9f3cb2b3ec19131e5ce582" target="_blank" rel="noopener">changed</a> <code>lora-scripts</code> to add new parameters about <code>LyCORIS</code>. Note here that <code>sd-scripts</code> is in the <code>lora-scripts</code> directory, but the dataset directory in the run configuration seems need to add <code>..</code> (parent directory) (is the work directory of <code>sd-scripts</code> runtime its own home directory?) .</p>
<h2 id="conclusion"><a href="#conclusion" class="anchor-link">#</a><a href="#contents:conclusion" class="headings">Conclusion</a></h2>
<p>It would be nice if I could use techniques like <a href="https://distill.pub/2017/feature-visualization/" target="_blank" rel="noopener">Feature Visualization</a> (for image task models) or token clustering (for text task models) to visually study the inside of my LoRA model, I&rsquo;m curious about which features it learns from Maho. It seems that there is no ready-made tools based on the stable diffusion model, so I gave up. And I&rsquo;m still not quite sure how caption works, but I can&rsquo;t find any more explanation online. If you know the relevant material, please do not hesitate to give advice.</p>
<p>I wish everyone can fine-tune their waifus successfully, set our hopes(or delusions?) free.</p>

            </div>

            
    
    
        <ul class="post-copyright">
            <li class="copyright-item author"><span class="copyright-item-text">Author: </span>: <a href="https://boholder.github.io/" class="p-author h-card" target="_blank" rel="noopener">BoHolder</a></li>
            
                
                
                
                
                <li class="copyright-item link"><span class="copyright-item-text">Link: </span>: <a href="/en-us/blogs/train-a-lora-model-for-an-anime-character/" target="_blank" rel="noopener">https://boholder.github.io/en-us/blogs/train-a-lora-model-for-an-anime-character/</a></li>
            
            <li class="copyright-item license"><span class="copyright-item-text">License: </span>: <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a></li>
            
        </ul>
    



        </article>

        

        
    <div class="updated-badge-container">
        <span title="Updated @ 2023-03-25 14:23:00 CST" style="cursor:help">

<svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2023-03-25</text><text x="915" y="140" textLength="650" transform="scale(.1)">2023-03-25</text></g></svg>
        </span></div>



        
    
        <div class="post-gitinfo">
            
            
                <div class="post-gitinfo-right">
                    
                        
                            <div class="gitinfo-item feedback">
                                <a href="https://github.com/boholder/boholder.github.io/issues" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" class="icon feedback-icon"><path d="M202.021 0C122.202 0 70.503 32.703 29.914 91.026c-7.363 10.58-5.093 25.086 5.178 32.874l43.138 32.709c10.373 7.865 25.132 6.026 33.253-4.148 25.049-31.381 43.63-49.449 82.757-49.449 30.764 0 68.816 19.799 68.816 49.631 0 22.552-18.617 34.134-48.993 51.164-35.423 19.86-82.299 44.576-82.299 106.405V320c0 13.255 10.745 24 24 24h72.471c13.255 0 24-10.745 24-24v-5.773c0-42.86 125.268-44.645 125.268-160.627C377.504 66.256 286.902 0 202.021 0zM192 373.459c-38.196 0-69.271 31.075-69.271 69.271 0 38.195 31.075 69.27 69.271 69.27s69.271-31.075 69.271-69.271-31.075-69.27-69.271-69.27z"/></svg>Feedback</a>
                            </div>
                        
                    
                    
                </div>
            
        </div>
    



        <div class="post-share">

        

        <div class="share-items">

            
                <div class="share-item twitter">
                    
                    <a href="https://twitter.com/share?url=https://boholder.github.io/en-us/blogs/train-a-lora-model-for-an-anime-character/&amp;text=Train%20a%20LoRA%20model%20for%20an%20anime%20character&amp;hashtags=StableDiffusion,&amp;via=%25!s%28%3cnil%3e%29" title="Share on Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon twitter-icon"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a>
                </div>
            

            
                <div class="share-item facebook">
                    
                    <a href="https://www.facebook.com/sharer/sharer.php?u=https://boholder.github.io/en-us/blogs/train-a-lora-model-for-an-anime-character/&amp;hashtag=%23StableDiffusion" title="Share on Facebook" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon facebook-icon"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg></a>
                </div>
            

            

            
                <div class="share-item telegram">
                    
                    <a href="https://t.me/share/url?url=https://boholder.github.io/en-us/blogs/train-a-lora-model-for-an-anime-character/&amp;text=Train%20a%20LoRA%20model%20for%20an%20anime%20character" title="Share on Telegram" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon telegram-icon"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9l-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"/></svg></a>
                </div>
            

            

            

            

            

            
                <div class="share-item qrcode">
                    <div class="qrcode-container" title="Share via QR Code"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qrcode-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg><div id="qrcode-img"></div>
                    </div>
                    <script src="https://cdn.jsdelivr.net/npm/qrcode-generator@1.4.4/qrcode.min.js"></script>

<script>
    const typeNumber = 0;
    const errorCorrectionLevel = 'L';
    const qr = qrcode(typeNumber, errorCorrectionLevel);
    qr.addData('https:\/\/boholder.github.io\/en-us\/blogs\/train-a-lora-model-for-an-anime-character\/');
    qr.make();
    document.getElementById('qrcode-img').innerHTML = qr.createImgTag();
</script>

                </div>
            

        </div>

    </div>




        
    
    



        
    
        <div class="post-tags">
            
                
                
                
                
                    
                    <a href="/en-us/tags/stablediffusion/" rel="tag" class="post-tags-link"><svg class="icon tag-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 28 36"><path d="M24 0H4C1.79 0 .02 1.79.02 4L0 36l14-6 14 6V4c0-2.21-1.79-4-4-4z"></path></svg>StableDiffusion</a>
                
            
        </div>
    



        
    <footer class="minimal-footer">
        
            <div class="post-tag"><a href="/en-us/tags/stablediffusion/" rel="tag" class="post-tag-link">#stablediffusion</a></div>
        
        
        
            
                <div class="post-category">
                    <a href="/en-us/categories/fun/" class="post-category-link">fun</a>
                </div>
            
        
    </footer>



        


        
    
        
        
    
    
    
    
        <ul class="post-nav">
            
            
                <li class="post-nav-next">
                    <a href="/en-us/blogs/minecraft-tree-house/" rel="next">Introducing a Minecraft treehouse I built (contains a total of 200MBsize images) &gt;</a>
                </li>
            
        </ul>
    



        
    

        

        

        

        
            <div id="utterances"></div>
        

        
    



    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            
    <footer id="footer" class="footer">
        <div class="footer-inner">
            <div class="site-info">©&nbsp;2019–2023&nbsp;<svg  xmlns="http://www.w3.org/2000/svg" height="16" width="16" id="glider_svg" ><circle style="fill:gray;stroke-width:0;" cx="8.3" cy="3" r="2"/><circle style="fill:gray;stroke-width:0;" cx="13.6" cy="8.3" r="2"/><circle style="fill:gray;stroke-width:0;" cx="13.6" cy="13.6" r="2"/><circle style="fill:gray;stroke-width:0;" cx="8.3" cy="13.6" r="2"/><circle style="fill:gray;stroke-width:0;" cx="3" cy="13.6" r="2"/></svg>&nbsp;BoHolder</div><div class="powered-by">Powered by <a href="https://github.com/gohugoio/hugo" target="_blank" rel="noopener">Hugo</a> | Theme is <a href="https://github.com/reuixiy/hugo-theme-meme" target="_blank" rel="noopener">MemE</a></div><div class="site-copyright"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a></div>

            
    
        <ul class="socials"><li class="socials-item">
                    <a href="/rss.xml" target="_blank" rel="external noopener" title="RSS"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M19.199 24C19.199 13.467 10.533 4.8 0 4.8V0c13.165 0 24 10.835 24 24h-4.801zM3.291 17.415c1.814 0 3.293 1.479 3.293 3.295 0 1.813-1.485 3.29-3.301 3.29C1.47 24 0 22.526 0 20.71s1.475-3.294 3.291-3.295zM15.909 24h-4.665c0-6.169-5.075-11.245-11.244-11.245V8.09c8.727 0 15.909 7.184 15.909 15.91z"/></svg></a>
                </li><li class="socials-item">
                    <a href="mailto:bottleholder@anche.no" target="_blank" rel="external noopener" title="Email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M464 64H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 400V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V400H48z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://github.com/boholder" target="_blank" rel="external noopener" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>
                </li></ul>
    



            
        </div>
    </footer>


        </div>
        

        






    

        

        

        
            <script>
    function loadComments() {
        (function() {
            const utterances = document.getElementById("utterances");
            if (!utterances) {
                return;
            }
            const script = document.createElement('script');
            script.src = 'https:\/\/utteranc.es\/client.js';
            script.async = true;
            script.crossOrigin = 'anonymous';
            script.setAttribute('repo', 'boholder\/boholder.github.io');
            script.setAttribute('issue-term', 'pathname');
            const isDark = getCurrentTheme() === 'dark';
        if (isDark) {
            script.setAttribute('theme', 'photon-dark');
        } else {
            script.setAttribute('theme', 'github-light');
        }
            
                script.setAttribute('label', 'en');
            
            utterances.appendChild(script);
        })();
    }
</script>
        

        

    



    <script src="https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js"></script>

<script>
    let imgNodes = document.querySelectorAll('div.post-body img');
    imgNodes = Array.from(imgNodes).filter(node => node.parentNode.tagName !== "A");

    mediumZoom(imgNodes, {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>






    </body>
</html>
